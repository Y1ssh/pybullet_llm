# PyBullet LLM Robotics

A comprehensive robotics simulation system that enables natural language control of robots using PyBullet physics simulation and multiple Large Language Model (LLM) providers.

## ğŸš€ Features

- **Natural Language Robot Control**: Control robots using everyday language
- **Multi-LLM Support**: Switch between Anthropic Claude, OpenAI GPT, and Google Gemini
- **Physics Simulation**: Realistic robot simulation using PyBullet
- **Computer Vision**: Object detection and manipulation using OpenCV
- **KUKA iiwa Robot**: Complete 7-DOF robot arm implementation
- **Interactive Chat Interface**: Real-time conversation with your robot
- **Safety Features**: Collision detection, workspace limits, emergency stop
- **Provider Comparison**: Compare responses from different LLM providers

## ğŸ› ï¸ Installation

### Prerequisites

- Python 3.9+
- Virtual environment (recommended)

### Step 1: Clone and Setup

```bash
git clone <repository-url>
cd pybullet_llm_robotics
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### Step 2: Install Dependencies

```bash
pip install -r requirements.txt
```

### Step 3: Configure API Keys

Copy the environment configuration:

```bash
cp env.example .env
```

Edit `.env` and add your API keys:

```bash
# API Keys
ANTHROPIC_API_KEY=sk-ant-your-key-here
OPENAI_API_KEY=sk-your-key-here
GOOGLE_API_KEY=your-google-gemini-key-here

# Default settings
DEFAULT_LLM_PROVIDER=anthropic
```

## ğŸ® Quick Start

### Interactive Chat Mode

Start the interactive robot control system:

```bash
cd scripts
python interactive_chat.py
```

Or with specific options:

```bash
python interactive_chat.py --provider openai --no-gui
```

### Basic Commands

Once the system is running, try these commands:

```
ğŸ¤ You: scan the environment
ğŸ¤ You: pick up the red cube
ğŸ¤ You: move to position 0.3, 0.4, 0.5
ğŸ¤ You: place the object on the table
ğŸ¤ You: switch to OpenAI
ğŸ¤ You: compare LLM responses for "pick up the blue sphere"
```

## ğŸ“š Usage Examples

### Basic Robot Control

```python
from src.core.environment import RobotEnvironment
from src.robots.kuka_iiwa import KukaIiwa
from src.llm.llm_controller import LLMRobotController
from src.core.sensors import CameraSystem

# Setup environment
env = RobotEnvironment(gui_mode=True)
env.setup_physics()

# Create robot
robot = KukaIiwa()
robot.load_robot(env.physics_client)

# Setup camera and controller
camera = CameraSystem(env)
controller = LLMRobotController(robot, env, camera)

# Control with natural language
response = controller.chat_with_robot("Move the robot to the red cube")
print(response)
```

### Multi-LLM Provider Usage

```python
# Switch between providers
controller.switch_llm_provider("openai")
controller.switch_llm_provider("google")

# Compare responses
comparison = controller.compare_llm_responses("How should I pick up the fragile glass?")
print(comparison)
```

### Computer Vision Integration

```python
from src.vision.object_detection import ObjectDetector

detector = ObjectDetector()
rgb_image = camera.capture_rgb_image()
objects = detector.detect_objects(rgb_image)

for obj in objects:
    print(f"Found {obj['color']} {obj['shape']} at {obj['center']}")
```

## ğŸ—ï¸ Architecture

```
pybullet_llm_robotics/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/              # Core robotics components
â”‚   â”‚   â”œâ”€â”€ robot_arm.py   # Base robot arm class
â”‚   â”‚   â”œâ”€â”€ environment.py # PyBullet environment
â”‚   â”‚   â””â”€â”€ sensors.py     # Camera and sensors
â”‚   â”œâ”€â”€ llm/               # LLM integration
â”‚   â”‚   â”œâ”€â”€ llm_controller.py  # Main LLM controller
â”‚   â”‚   â””â”€â”€ robot_tools.py     # LangChain tools
â”‚   â”œâ”€â”€ vision/            # Computer vision
â”‚   â”‚   â””â”€â”€ object_detection.py
â”‚   â”œâ”€â”€ robots/            # Robot implementations
â”‚   â”‚   â”œâ”€â”€ kuka_iiwa.py   # KUKA iiwa robot
â”‚   â”‚   â””â”€â”€ llm_provider_manager.py
â”‚   â””â”€â”€ utils/             # Utilities
â”‚       â””â”€â”€ config.py      # Configuration management
â”œâ”€â”€ scripts/               # Executable scripts
â”‚   â””â”€â”€ interactive_chat.py
â”œâ”€â”€ data/                  # Robot models and configs
â”œâ”€â”€ tests/                 # Unit tests
â””â”€â”€ examples/              # Demo implementations
```

## ğŸ¯ Supported Commands

### Movement Commands
- "Move the robot to position X, Y, Z"
- "Go to the center of the table"
- "Reset robot to home position"

### Object Manipulation
- "Pick up the [color] [shape]"
- "Grab the object on the left"
- "Place the object at position X, Y, Z"
- "Put it on the table"

### Environment Interaction
- "Scan the environment"
- "What objects can you see?"
- "Find the red cube"
- "Count the objects"

### LLM Provider Management
- "Switch to OpenAI"
- "Use Google Gemini"
- "Compare all providers for this task"

### System Commands
- "help" - Show available commands
- "status" - Display system status
- "providers" - List LLM providers
- "quit" - Exit the system

## ğŸ”§ Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `DEFAULT_LLM_PROVIDER` | Default LLM provider | `anthropic` |
| `LLM_TEMPERATURE` | LLM response temperature | `0.1` |
| `MAX_TOKENS` | Maximum response tokens | `1000` |
| `CAMERA_WIDTH` | Camera image width | `640` |
| `CAMERA_HEIGHT` | Camera image height | `480` |
| `DEBUG_MODE` | Enable debug logging | `false` |

### Robot Configuration

```python
from src.utils.config import robot_config

robot_config.robot_type = "kuka_iiwa"
robot_config.base_position = [0, 0, 0]
robot_config.home_position = [0, 0, 0, -1.57, 0, 1.57, 0]
```

## ğŸ§ª Testing

Run the test suite:

```bash
python -m pytest tests/ -v
```

Run specific tests:

```bash
python -m pytest tests/test_robot_arm.py -v
python -m pytest tests/test_llm_integration.py -v
```

## ğŸ” Troubleshooting

### Common Issues

**1. "No LLM providers available"**
- Check your API keys in `.env`
- Ensure required packages are installed: `pip install langchain-anthropic langchain-openai langchain-google-genai`

**2. "Robot URDF not found"**
- The system will create a simple KUKA URDF automatically
- For custom URDFs, place them in `data/robot_models/kuka_iiwa/`

**3. "Camera not working"**
- Ensure OpenCV is installed: `pip install opencv-python`
- Check if running in headless mode (use `--no-gui` flag)

**4. "PyBullet connection failed"**
- Try running without GUI: `python interactive_chat.py --no-gui`
- Check PyBullet installation: `pip install pybullet`

### Debug Mode

Enable debug logging:

```bash
python interactive_chat.py --log-level DEBUG
```

## ğŸ“ˆ Performance Tips

1. **Use appropriate LLM providers**:
   - Anthropic Claude: Best for complex reasoning
   - OpenAI GPT: Balanced performance
   - Google Gemini: Fast responses

2. **Optimize camera settings**:
   ```python
   env.setup_camera(width=320, height=240)  # Lower resolution for speed
   ```

3. **Reduce simulation complexity**:
   ```python
   env.timestep = 0.01  # Larger timestep for faster simulation
   ```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make your changes
4. Add tests for new functionality
5. Run tests: `python -m pytest`
6. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- PyBullet team for the physics simulation engine
- LangChain for LLM integration framework
- OpenAI, Anthropic, and Google for LLM APIs
- OpenCV community for computer vision tools

## ğŸ“ Support

For questions and support:
- Create an issue on GitHub
- Check the documentation in `/docs`
- Review example implementations in `/examples`

---

**Happy Robot Programming! ğŸ¤–** 